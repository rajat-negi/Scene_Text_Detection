{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scene_final_pipeline_deployment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owNig8_B5CiE",
        "outputId": "c34226d4-f339-474b-c38c-c3779b719a65"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrNdrdjG5VPu",
        "outputId": "7fd5cc57-24be-4972-87f0-fdace656410a"
      },
      "source": [
        "pip install autocorrect"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30 kB 38.9 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 153 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 358 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 512 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 552 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 593 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 622 kB 13.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.5.0-py3-none-any.whl size=621853 sha256=29e4f25a9b765fda0ea9550e249cc2446ef92b3a87fefbcca7949bba4288d9eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/8e/bd/f6fd900a056a031bf710a00bca338d86f43b83f0c25ab5242f\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am154FGu5jYu",
        "outputId": "92d201cc-835f-4b9a-c564-2b2115354171"
      },
      "source": [
        "pip install pyngrok"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.0.5.tar.gz (745 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 92 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 112 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 122 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 133 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 143 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 153 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 163 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 174 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 184 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 194 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 204 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 215 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 225 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 235 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 245 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 256 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 266 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 276 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 286 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 296 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 307 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 317 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 337 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 348 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 358 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 368 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 378 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 389 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 399 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 409 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 419 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 430 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 440 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 450 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 460 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 471 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 481 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 491 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 501 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 512 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 522 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 532 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 542 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 552 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 563 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 573 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 583 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 593 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 604 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 614 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 624 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 634 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 645 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 655 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 665 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 675 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 686 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 696 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 706 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 716 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 727 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 737 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 745 kB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.0.5-py3-none-any.whl size=19262 sha256=c62da9bb5624a22ce95c8c050a26561250c72b138df4e44e2a27f7027e77197d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/f7/72/35c95a53d15b91dd00df6cf1304d49a31ec5ed6f954c2d4e32\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI3meJiMX2kW",
        "outputId": "93cff106-12ff-4c22-dff5-b7fd6b4f1f15"
      },
      "source": [
        "!pip install -q streamlit"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 13.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 111 kB 78.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 63.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 74.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 122 kB 30.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 786 kB 47.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 368 kB 67.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.19 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.25.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQpKHO3L3tvH",
        "outputId": "e1659d22-2f2b-4889-f7dd-4fc183dacf4f"
      },
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, jsonify, request, redirect\n",
        "import joblib\n",
        "# https://www.tutorialspoint.com/flask\n",
        "import flask\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "from PIL import Image, ImagePath, ImageDraw\n",
        "import os\n",
        "import glob\n",
        "import csv\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import re\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import streamlit as st\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Reshape, Dense, GRU, Bidirectional\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import tensorflow.keras.backend as K\n",
        "import scipy.io as sio\n",
        "from shapely.geometry import Polygon\n",
        "from google.colab import files\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "colab_path = \"/content/drive/MyDrive/\"\n",
        "content_folder = \"/content/\"\n",
        "\n",
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')\n",
        "\n",
        "\n",
        "NUM_CLASSES = 22553\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "############################################ LOADING NECESSARY FUNCTIONS #############################################\n",
        "\n",
        "def load_annotation(p):\n",
        "    '''\n",
        "    load polygon coordinate and text from the text file for corresponding image\n",
        "    here p is name of image file whose cooresponding annotation we want \n",
        "    '''\n",
        "    text_polys = []\n",
        "    text_tags = []\n",
        "    if not os.path.exists(p):\n",
        "        return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.int32)\n",
        "    with open(p, 'r', encoding = 'utf-8-sig') as f:\n",
        "        #Reading our data\n",
        "        reader = f.readlines()\n",
        "        for line in reader:            \n",
        "            line= line.replace(\"\\n\", \"\")\n",
        "            line = line.replace('\\xef\\xbb\\bf', '')\n",
        "            line = line.replace('\\xe2\\x80\\x8d', '')\n",
        "            line = line.strip()\n",
        "            line = line.split(',')\n",
        "            #Taking word level locations to plot bounding box i.e,377,117,463,117,465,130,378,130\n",
        "            x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))\n",
        "            #Appending coordinates\n",
        "            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])\n",
        "            \n",
        "            \n",
        "            if len(line) > 9:\n",
        "                label = \",\".join(line[9:])\n",
        "            else:\n",
        "                label = line[-1]\n",
        "                \n",
        "            #Appending words and ###\n",
        "            if label == '*' or label == '###' or label == '':\n",
        "                text_tags.append(None)\n",
        "            else:\n",
        "                text_tags.append(label)\n",
        "\n",
        "        return np.array(text_polys, dtype=np.float32), np.array(text_tags)\n",
        "\n",
        "\n",
        "# This Function is used to calculate AREA of polygon\n",
        "def polygon_area(poly):\n",
        "    '''\n",
        "    compute area of a polygon\n",
        "    '''\n",
        "    edge = [\n",
        "        (poly[1][0] - poly[0][0]) * (poly[1][1] + poly[0][1]),\n",
        "        (poly[2][0] - poly[1][0]) * (poly[2][1] + poly[1][1]),\n",
        "        (poly[3][0] - poly[2][0]) * (poly[3][1] + poly[2][1]),\n",
        "        (poly[0][0] - poly[3][0]) * (poly[0][1] + poly[3][1])\n",
        "    ]\n",
        "    return np.sum(edge)/2.\n",
        "\n",
        "\n",
        "#https://github.com/argman/EAST/blob/master/icdar.py\n",
        "\n",
        "def is_polygon(poly):\n",
        "    for i in range(3):\n",
        "        p0 = poly[i]\n",
        "\n",
        "        p1 = poly[(i + 1) % 4]\n",
        "        p2 = poly[(i + 2) % 4]\n",
        "\n",
        "        if p0[0] == p1[0] and p1[1] == p0[1]:\n",
        "            return False\n",
        "        if p0[0] == p2[0] and p2[1] == p0[1]:\n",
        "            return False\n",
        "        if p1[0] == p2[0] and p1[1] == p2[1]:\n",
        "            return False\n",
        "\n",
        "        if p0[0] == p1[0]:\n",
        "            if p1[0] == p2[0]:\n",
        "                return False\n",
        "        else:\n",
        "            if p1[0] != p2[0]:\n",
        "                k1 = (p1[1] - p0[1]) / (p1[0] - p0[0])\n",
        "                k2 = (p2[1] - p1[1]) / (p2[0] - p1[0])\n",
        "                if abs(k1 - k2) < 1e-6:\n",
        "                    return False\n",
        "                else:\n",
        "                    if p1[1] == p2[1]:\n",
        "                        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "#3\n",
        "# This function is used to discard invalid polygons and check direction of them\n",
        "# https://github.com/Masao-Taketani/FOTS_OCR\n",
        "\n",
        "\n",
        "#https://github.com/argman/EAST/blob/master/icdar.py\n",
        "def check_and_validate_polys(polys, tags, xxx_todo_changeme):\n",
        "    '''\n",
        "    check so that the text poly is in the same direction,\n",
        "    and also filter some invalid polygons\n",
        "    :param polys:\n",
        "    :param tags:\n",
        "    :return:\n",
        "    '''\n",
        "    #Taking height and width of the image\n",
        "    (h, w) = xxx_todo_changeme\n",
        "    if polys.shape[0] == 0:\n",
        "        return polys\n",
        "    polys[:, :, 0] = np.clip(polys[:, :, 0], 0, w-1)\n",
        "    polys[:, :, 1] = np.clip(polys[:, :, 1], 0, h-1)\n",
        "\n",
        "    validated_polys = []\n",
        "    validated_tags = []\n",
        "    for poly, tag in zip(polys, tags):\n",
        "        p_area = polygon_area(poly)\n",
        "        \n",
        "        # memory error after hitting not a polygon !!!\n",
        "        if is_polygon(poly) is False:\n",
        "            #print(\"not a polygon: \", poly)\n",
        "            continue\n",
        "        \n",
        "        if abs(p_area) < 1:\n",
        "#             print('invalid poly')\n",
        "            continue\n",
        "        if p_area > 0:\n",
        "#             print('poly in wrong direction')\n",
        "            poly = poly[(0, 3, 2, 1), :]\n",
        "        validated_polys.append(poly)\n",
        "        validated_tags.append(tag)\n",
        "        \n",
        "    return np.array(validated_polys), validated_tags\n",
        "\n",
        "#https://github.com/argman/EAST/blob/master/icdar.py\n",
        "\n",
        "#This function is implementation of Polygon Shrinkage Algorithm \n",
        "def shrink_poly(poly, r):\n",
        "    '''\n",
        "    fit a poly inside the origin poly\n",
        "    used for generate the score map\n",
        "    '''\n",
        "    # shrink ratio\n",
        "    R = 0.3\n",
        "    # find the longer pair\n",
        "    if np.linalg.norm(poly[0] - poly[1]) + np.linalg.norm(poly[2] - poly[3]) > \\\n",
        "                    np.linalg.norm(poly[0] - poly[3]) + np.linalg.norm(poly[1] - poly[2]):\n",
        "        # first move (p0, p1), (p2, p3), then (p0, p3), (p1, p2)\n",
        "        ## p0, p1\n",
        "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
        "        poly[0][0] += R * r[0] * np.cos(theta)\n",
        "        poly[0][1] += R * r[0] * np.sin(theta)\n",
        "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
        "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
        "        ## p2, p3\n",
        "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
        "        poly[3][0] += R * r[3] * np.cos(theta)\n",
        "        poly[3][1] += R * r[3] * np.sin(theta)\n",
        "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
        "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
        "        ## p0, p3\n",
        "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
        "        poly[0][0] += R * r[0] * np.sin(theta)\n",
        "        poly[0][1] += R * r[0] * np.cos(theta)\n",
        "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
        "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
        "        ## p1, p2\n",
        "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
        "        poly[1][0] += R * r[1] * np.sin(theta)\n",
        "        poly[1][1] += R * r[1] * np.cos(theta)\n",
        "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
        "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
        "    else:\n",
        "        ## p0, p3\n",
        "        # print poly\n",
        "        theta = np.arctan2((poly[3][0] - poly[0][0]), (poly[3][1] - poly[0][1]))\n",
        "        poly[0][0] += R * r[0] * np.sin(theta)\n",
        "        poly[0][1] += R * r[0] * np.cos(theta)\n",
        "        poly[3][0] -= R * r[3] * np.sin(theta)\n",
        "        poly[3][1] -= R * r[3] * np.cos(theta)\n",
        "        ## p1, p2\n",
        "        theta = np.arctan2((poly[2][0] - poly[1][0]), (poly[2][1] - poly[1][1]))\n",
        "        poly[1][0] += R * r[1] * np.sin(theta)\n",
        "        poly[1][1] += R * r[1] * np.cos(theta)\n",
        "        poly[2][0] -= R * r[2] * np.sin(theta)\n",
        "        poly[2][1] -= R * r[2] * np.cos(theta)\n",
        "        ## p0, p1\n",
        "        theta = np.arctan2((poly[1][1] - poly[0][1]), (poly[1][0] - poly[0][0]))\n",
        "        poly[0][0] += R * r[0] * np.cos(theta)\n",
        "        poly[0][1] += R * r[0] * np.sin(theta)\n",
        "        poly[1][0] -= R * r[1] * np.cos(theta)\n",
        "        poly[1][1] -= R * r[1] * np.sin(theta)\n",
        "        ## p2, p3\n",
        "        theta = np.arctan2((poly[2][1] - poly[3][1]), (poly[2][0] - poly[3][0]))\n",
        "        poly[3][0] += R * r[3] * np.cos(theta)\n",
        "        poly[3][1] += R * r[3] * np.sin(theta)\n",
        "        poly[2][0] -= R * r[2] * np.cos(theta)\n",
        "        poly[2][1] -= R * r[2] * np.sin(theta)\n",
        "    return poly\n",
        "\n",
        "\n",
        "\n",
        "#Compute distance between p1-p2 and p3\n",
        "def point_dist_to_line(p1, p2, p3):\n",
        "    '''compute the distance from p3 to p1-p2'''\n",
        "    return np.linalg.norm(np.cross(p2 - p1, p1 - p3)) / np.linalg.norm(p2 - p1)\n",
        "\n",
        "#Find equation of line using two 2D points p1 and p2\n",
        "def fit_line(p1, p2):\n",
        "    '''fit a line ax+by+c = 0'''\n",
        "    if p1[0] == p1[1]:\n",
        "        return [1., 0., -p1[0]]\n",
        "    else:\n",
        "        [k, b] = np.polyfit(p1, p2, deg=1)\n",
        "        return [k, -1., b]\n",
        "\n",
        "#Find Intersection poitn of 2 lines\n",
        "def line_cross_point(line1, line2):\n",
        "    '''line1 0= ax+by+c, compute the cross point of line1 and line2'''\n",
        "    if line1[0] != 0 and line1[0] == line2[0]:\n",
        "        print('Cross point does not exist')\n",
        "        return None\n",
        "    if line1[0] == 0 and line2[0] == 0:\n",
        "        print('Cross point does not exist')\n",
        "        return None\n",
        "    if line1[1] == 0:\n",
        "        x = -line1[2]\n",
        "        y = line2[0] * x + line2[2]\n",
        "    elif line2[1] == 0:\n",
        "        x = -line2[2]\n",
        "        y = line1[0] * x + line1[2]\n",
        "    else:\n",
        "        k1, _, b1 = line1\n",
        "        k2, _, b2 = line2\n",
        "        x = -(b1-b2)/(k1-k2)\n",
        "        y = k1*x + b1\n",
        "    return np.array([x, y], dtype=np.float32)\n",
        "\n",
        "#Get Equation of line that is perpendicular to line passing through a point\n",
        "def line_verticle(line, point):\n",
        "    '''get the verticle line from line across point'''\n",
        "    if line[1] == 0:\n",
        "        verticle = [0, -1, point[1]]\n",
        "    else:\n",
        "        if line[0] == 0:\n",
        "            verticle = [1, 0, -point[0]]\n",
        "        else:\n",
        "            verticle = [-1./line[0], -1, point[1] - (-1/line[0] * point[0])]\n",
        "    return verticle\n",
        "\n",
        "# Convert a parallelogram to rectangle\n",
        "def rectangle_from_parallelogram(poly):\n",
        "    '''\n",
        "    fit a rectangle from a parallelogram\n",
        "    '''\n",
        "    p0, p1, p2, p3 = poly\n",
        "    angle_p0 = np.arccos(np.dot(p1-p0, p3-p0)/(np.linalg.norm(p0-p1) * np.linalg.norm(p3-p0)))\n",
        "    if angle_p0 < 0.5 * np.pi:\n",
        "        if np.linalg.norm(p0 - p1) > np.linalg.norm(p0-p3):\n",
        "            # p0 and p2\n",
        "            ## p0\n",
        "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
        "            p2p3_verticle = line_verticle(p2p3, p0)\n",
        "\n",
        "            new_p3 = line_cross_point(p2p3, p2p3_verticle)\n",
        "            ## p2\n",
        "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
        "            p0p1_verticle = line_verticle(p0p1, p2)\n",
        "\n",
        "            new_p1 = line_cross_point(p0p1, p0p1_verticle)\n",
        "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
        "        else:\n",
        "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
        "            p1p2_verticle = line_verticle(p1p2, p0)\n",
        "\n",
        "            new_p1 = line_cross_point(p1p2, p1p2_verticle)\n",
        "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
        "            p0p3_verticle = line_verticle(p0p3, p2)\n",
        "\n",
        "            new_p3 = line_cross_point(p0p3, p0p3_verticle)\n",
        "            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n",
        "    else:\n",
        "        if np.linalg.norm(p0-p1) > np.linalg.norm(p0-p3):\n",
        "            # p1 and p3\n",
        "            ## p1\n",
        "            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n",
        "            p2p3_verticle = line_verticle(p2p3, p1)\n",
        "\n",
        "            new_p2 = line_cross_point(p2p3, p2p3_verticle)\n",
        "            ## p3\n",
        "            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
        "            p0p1_verticle = line_verticle(p0p1, p3)\n",
        "\n",
        "            new_p0 = line_cross_point(p0p1, p0p1_verticle)\n",
        "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n",
        "        else:\n",
        "            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
        "            p0p3_verticle = line_verticle(p0p3, p1)\n",
        "\n",
        "            new_p0 = line_cross_point(p0p3, p0p3_verticle)\n",
        "            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
        "            p1p2_verticle = line_verticle(p1p2, p3)\n",
        "\n",
        "            new_p2 = line_cross_point(p1p2, p1p2_verticle)\n",
        "            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n",
        "\n",
        "#Sorting a rectangle to get all point in clockwies manner\n",
        "def sort_rectangle(poly):\n",
        "    '''sort the four coordinates of the polygon, points in poly should be sorted clockwise'''\n",
        "    # First find the lowest point\n",
        "    p_lowest = np.argmax(poly[:, 1])\n",
        "    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\n",
        "        # if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\n",
        "        p0_index = np.argmin(np.sum(poly, axis=1))\n",
        "        p1_index = (p0_index + 1) % 4\n",
        "        p2_index = (p0_index + 2) % 4\n",
        "        p3_index = (p0_index + 3) % 4\n",
        "        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\n",
        "    else:\n",
        "        # find the point that sits right to the lowest point\n",
        "        p_lowest_right = (p_lowest - 1) % 4\n",
        "        p_lowest_left = (p_lowest + 1) % 4\n",
        "        angle = np.arctan(-(poly[p_lowest][1] - poly[p_lowest_right][1])/(poly[p_lowest][0] - poly[p_lowest_right][0]))\n",
        "        # assert angle > 0\n",
        "        if angle <= 0:\n",
        "            print(angle, poly[p_lowest], poly[p_lowest_right])\n",
        "        if angle/np.pi * 180 > 45:\n",
        "            #this point is p2\n",
        "            p2_index = p_lowest\n",
        "            p1_index = (p2_index - 1) % 4\n",
        "            p0_index = (p2_index - 2) % 4\n",
        "            p3_index = (p2_index + 1) % 4\n",
        "            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi/2 - angle)\n",
        "        else:\n",
        "            # this point is p3\n",
        "            p3_index = p_lowest\n",
        "            p0_index = (p3_index + 1) % 4\n",
        "            p1_index = (p3_index + 2) % 4\n",
        "            p2_index = (p3_index + 3) % 4\n",
        "            return poly[[p0_index, p1_index, p2_index, p3_index]], angle\n",
        "\n",
        "\n",
        "def restore_rectangle_rbox(origin, geometry):\n",
        "    ''' Resotre rectangle tbox'''\n",
        "    d = geometry[:, :4]\n",
        "    angle = geometry[:, 4]\n",
        "    # for angle > 0\n",
        "    origin_0 = origin[angle >= 0]\n",
        "    d_0 = d[angle >= 0]\n",
        "    angle_0 = angle[angle >= 0]\n",
        "    if origin_0.shape[0] > 0:\n",
        "        p = np.array([np.zeros(d_0.shape[0]), -d_0[:, 0] - d_0[:, 2],\n",
        "                      d_0[:, 1] + d_0[:, 3], -d_0[:, 0] - d_0[:, 2],\n",
        "                      d_0[:, 1] + d_0[:, 3], np.zeros(d_0.shape[0]),\n",
        "                      np.zeros(d_0.shape[0]), np.zeros(d_0.shape[0]),\n",
        "                      d_0[:, 3], -d_0[:, 2]])\n",
        "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
        "\n",
        "        rotate_matrix_x = np.array([np.cos(angle_0), np.sin(angle_0)]).transpose((1, 0))\n",
        "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
        "\n",
        "        rotate_matrix_y = np.array([-np.sin(angle_0), np.cos(angle_0)]).transpose((1, 0))\n",
        "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
        "\n",
        "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "\n",
        "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
        "\n",
        "        p3_in_origin = origin_0 - p_rotate[:, 4, :]\n",
        "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
        "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
        "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
        "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
        "\n",
        "        new_p_0 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
        "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
        "    else:\n",
        "        new_p_0 = np.zeros((0, 4, 2))\n",
        "    # for angle < 0\n",
        "    origin_1 = origin[angle < 0]\n",
        "    d_1 = d[angle < 0]\n",
        "    angle_1 = angle[angle < 0]\n",
        "    if origin_1.shape[0] > 0:\n",
        "        p = np.array([-d_1[:, 1] - d_1[:, 3], -d_1[:, 0] - d_1[:, 2],\n",
        "                      np.zeros(d_1.shape[0]), -d_1[:, 0] - d_1[:, 2],\n",
        "                      np.zeros(d_1.shape[0]), np.zeros(d_1.shape[0]),\n",
        "                      -d_1[:, 1] - d_1[:, 3], np.zeros(d_1.shape[0]),\n",
        "                      -d_1[:, 1], -d_1[:, 2]])\n",
        "        p = p.transpose((1, 0)).reshape((-1, 5, 2))  # N*5*2\n",
        "\n",
        "        rotate_matrix_x = np.array([np.cos(-angle_1), -np.sin(-angle_1)]).transpose((1, 0))\n",
        "        rotate_matrix_x = np.repeat(rotate_matrix_x, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))  # N*5*2\n",
        "\n",
        "        rotate_matrix_y = np.array([np.sin(-angle_1), np.cos(-angle_1)]).transpose((1, 0))\n",
        "        rotate_matrix_y = np.repeat(rotate_matrix_y, 5, axis=1).reshape(-1, 2, 5).transpose((0, 2, 1))\n",
        "\n",
        "        p_rotate_x = np.sum(rotate_matrix_x * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "        p_rotate_y = np.sum(rotate_matrix_y * p, axis=2)[:, :, np.newaxis]  # N*5*1\n",
        "\n",
        "        p_rotate = np.concatenate([p_rotate_x, p_rotate_y], axis=2)  # N*5*2\n",
        "\n",
        "        p3_in_origin = origin_1 - p_rotate[:, 4, :]\n",
        "        new_p0 = p_rotate[:, 0, :] + p3_in_origin  # N*2\n",
        "        new_p1 = p_rotate[:, 1, :] + p3_in_origin\n",
        "        new_p2 = p_rotate[:, 2, :] + p3_in_origin\n",
        "        new_p3 = p_rotate[:, 3, :] + p3_in_origin\n",
        "\n",
        "        new_p_1 = np.concatenate([new_p0[:, np.newaxis, :], new_p1[:, np.newaxis, :],\n",
        "                                  new_p2[:, np.newaxis, :], new_p3[:, np.newaxis, :]], axis=1)  # N*4*2\n",
        "    else:\n",
        "        new_p_1 = np.zeros((0, 4, 2))\n",
        "    return np.concatenate([new_p_0, new_p_1])\n",
        "\n",
        "\n",
        "#Some geometrical functions used in codes\n",
        "def restore_rectangle(origin, geometry):\n",
        "    return restore_rectangle_rbox(origin, geometry)\n",
        "\n",
        "def getRotateRect(box):\n",
        "    rect = cv2.minAreaRect(box)\n",
        "\n",
        "    angle=rect[2]  # angle = [-90, 0)\n",
        "    if angle < -45:\n",
        "        rect = (rect[0], (rect[1][0], rect[1][1]), rect[2])\n",
        "        angle += 90\n",
        "        size = (rect[1][1],rect[1][0])\n",
        "    else:\n",
        "        rect = (rect[0], (rect[1][0], rect[1][1]), rect[2])\n",
        "        size=rect[1]\n",
        "\n",
        "    box_ = cv2.boxPoints(rect)\n",
        "    return np.concatenate([rect[0], size]), angle, box_\n",
        "\n",
        "\n",
        "#These Functions are used to Generate ROI params like out box,crop box & angles that we use to crop text from image\n",
        "def generate_roiRotatePara(box, angle, expand_w = 60):\n",
        "    '''Generate all ROI Parameterts'''\n",
        "    p0_rect, p1_rect, p2_rect, p3_rect = box\n",
        "    cxy = (p0_rect + p2_rect) / 2.\n",
        "    size = np.array([np.linalg.norm(p0_rect - p1_rect), np.linalg.norm(p0_rect - p3_rect)])\n",
        "    rrect = np.concatenate([cxy, size])\n",
        "\n",
        "    box=np.array(box)\n",
        "\n",
        "    points=np.array(box, dtype=np.int32)\n",
        "    xmin=np.min(points[:,0])\n",
        "    xmax=np.max(points[:,0])\n",
        "    ymin=np.min(points[:,1])\n",
        "    ymax=np.max(points[:,1])\n",
        "    bbox = np.array([xmin, ymin, xmax, ymax])\n",
        "    if np.any(bbox < -expand_w):\n",
        "        return None\n",
        "    \n",
        "    rrect[:2] -= bbox[:2]\n",
        "    rrect[:2] -= rrect[2:] / 2\n",
        "    rrect[2:] += rrect[:2]\n",
        "\n",
        "    bbox[2:] -= bbox[:2]\n",
        "\n",
        "    rrect[::2] = np.clip(rrect[::2], 0, bbox[2])\n",
        "    rrect[1::2] = np.clip(rrect[1::2], 0, bbox[3])\n",
        "    rrect[2:] -= rrect[:2]\n",
        "    \n",
        "    return bbox.astype(np.int32), rrect.astype(np.int32), - angle\n",
        "\n",
        "def restore_roiRotatePara(box):\n",
        "    rectange, rotate_angle = sort_rectangle(box)\n",
        "    return generate_roiRotatePara(rectange, rotate_angle)\n",
        "\n",
        "#This function is used to generate geo_map,score_map, training_mask,corp_box,out_box,angle that we use while training model\n",
        "def generate_rbox(im_size, polys, tags):\n",
        "    '''Genrate score_map and geo_map for image'''\n",
        "    h, w = im_size\n",
        "    poly_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    score_map = np.zeros((h, w), dtype=np.uint8)\n",
        "    geo_map = np.zeros((h, w, 5), dtype=np.float32)\n",
        "\n",
        "    outBoxs = []\n",
        "    cropBoxs = []\n",
        "    angles = []\n",
        "    text_tags = []\n",
        "    recg_masks = []\n",
        "    # mask used during traning, to ignore some hard areas\n",
        "    training_mask = np.ones((h, w), dtype=np.uint8)\n",
        "    for poly_idx, poly_tag in enumerate(zip(polys, tags)):\n",
        "        poly = poly_tag[0]\n",
        "        #print(poly)\n",
        "        tag = poly_tag[1]\n",
        "        #print(tag)\n",
        "        r = [None, None, None, None]\n",
        "        for i in range(4):\n",
        "            r[i] = min(np.linalg.norm(poly[i] - poly[(i + 1) % 4]),\n",
        "                       np.linalg.norm(poly[i] - poly[(i - 1) % 4]))\n",
        "        # score map\n",
        "        shrinked_poly = shrink_poly(poly.copy(), r).astype(np.int32)[np.newaxis, :, :]\n",
        "        cv2.fillPoly(score_map, shrinked_poly, 1)\n",
        "        cv2.fillPoly(poly_mask, shrinked_poly, poly_idx + 1)\n",
        "\n",
        "        # if geometry == 'RBOX':\n",
        "        # generate a parallelogram for any combination of two vertices\n",
        "        fitted_parallelograms = []\n",
        "        for i in range(4):\n",
        "            p0 = poly[i]\n",
        "            p1 = poly[(i + 1) % 4]\n",
        "            p2 = poly[(i + 2) % 4]\n",
        "            p3 = poly[(i + 3) % 4]\n",
        "            edge = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n",
        "            backward_edge = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n",
        "            forward_edge = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n",
        "            if point_dist_to_line(p0, p1, p2) > point_dist_to_line(p0, p1, p3):\n",
        "                #  parallel lines through p2\n",
        "                if edge[1] == 0:\n",
        "                    edge_opposite = [1, 0, -p2[0]]\n",
        "                else:\n",
        "                    edge_opposite = [edge[0], -1, p2[1] - edge[0] * p2[0]]\n",
        "            else:\n",
        "                # after p3\n",
        "                if edge[1] == 0:\n",
        "                    edge_opposite = [1, 0, -p3[0]]\n",
        "                else:\n",
        "                    edge_opposite = [edge[0], -1, p3[1] - edge[0] * p3[0]]\n",
        "            # move forward edge\n",
        "            new_p0 = p0\n",
        "            new_p1 = p1\n",
        "            new_p2 = p2\n",
        "            new_p3 = p3\n",
        "            new_p2 = line_cross_point(forward_edge, edge_opposite)\n",
        "            if point_dist_to_line(p1, new_p2, p0) > point_dist_to_line(p1, new_p2, p3):\n",
        "                # across p0\n",
        "                if forward_edge[1] == 0:\n",
        "                    forward_opposite = [1, 0, -p0[0]]\n",
        "                else:\n",
        "                    forward_opposite = [forward_edge[0], -1, p0[1] - forward_edge[0] * p0[0]]\n",
        "            else:\n",
        "                # across p3\n",
        "                if forward_edge[1] == 0:\n",
        "                    forward_opposite = [1, 0, -p3[0]]\n",
        "                else:\n",
        "                    forward_opposite = [forward_edge[0], -1, p3[1] - forward_edge[0] * p3[0]]\n",
        "            new_p0 = line_cross_point(forward_opposite, edge)\n",
        "            new_p3 = line_cross_point(forward_opposite, edge_opposite)\n",
        "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
        "            # or move backward edge\n",
        "            new_p0 = p0\n",
        "            new_p1 = p1\n",
        "            new_p2 = p2\n",
        "            new_p3 = p3\n",
        "            new_p3 = line_cross_point(backward_edge, edge_opposite)\n",
        "            if point_dist_to_line(p0, p3, p1) > point_dist_to_line(p0, p3, p2):\n",
        "                # across p1\n",
        "                if backward_edge[1] == 0:\n",
        "                    backward_opposite = [1, 0, -p1[0]]\n",
        "                else:\n",
        "                    backward_opposite = [backward_edge[0], -1, p1[1] - backward_edge[0] * p1[0]]\n",
        "            else:\n",
        "                # across p2\n",
        "                if backward_edge[1] == 0:\n",
        "                    backward_opposite = [1, 0, -p2[0]]\n",
        "                else:\n",
        "                    backward_opposite = [backward_edge[0], -1, p2[1] - backward_edge[0] * p2[0]]\n",
        "            new_p1 = line_cross_point(backward_opposite, edge)\n",
        "            new_p2 = line_cross_point(backward_opposite, edge_opposite)\n",
        "            fitted_parallelograms.append([new_p0, new_p1, new_p2, new_p3, new_p0])\n",
        "        areas = [Polygon(t).area for t in fitted_parallelograms]\n",
        "        parallelogram = np.array(fitted_parallelograms[np.argmin(areas)][:-1], dtype=np.float32)\n",
        "        # sort thie polygon\n",
        "        parallelogram_coord_sum = np.sum(parallelogram, axis=1)\n",
        "        min_coord_idx = np.argmin(parallelogram_coord_sum)\n",
        "        parallelogram = parallelogram[\n",
        "            [min_coord_idx, (min_coord_idx + 1) % 4, (min_coord_idx + 2) % 4, (min_coord_idx + 3) % 4]]\n",
        "\n",
        "        rectange = rectangle_from_parallelogram(parallelogram)\n",
        "        rectange, rotate_angle = sort_rectangle(rectange)\n",
        "\n",
        "        p0_rect, p1_rect, p2_rect, p3_rect = rectange\n",
        "\n",
        "        # if the poly is too small, then ignore it during training\n",
        "        poly_h = min(np.linalg.norm(p0_rect - p3_rect), np.linalg.norm(p1_rect - p2_rect))\n",
        "        poly_w = min(np.linalg.norm(p0_rect - p1_rect), np.linalg.norm(p2_rect - p3_rect))\n",
        "\n",
        "        invaild = (min(poly_h, poly_w) < 6) or tag is None or (True and poly_h > poly_w * 2)\n",
        "\n",
        "        if invaild:\n",
        "            cv2.fillPoly(training_mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
        "        xy_in_poly = np.argwhere(poly_mask == (poly_idx + 1))\n",
        "        \n",
        "        if not invaild:\n",
        "            roiRotatePara = generate_roiRotatePara(rectange, rotate_angle)\n",
        "            if roiRotatePara:\n",
        "                outBox, cropBox, angle = roiRotatePara\n",
        "                if min(cropBox[2:]) > 6:\n",
        "                    w , h = cropBox[2:]\n",
        "                    textImgW = np.ceil(min(w / float(h) * 32, 256) / 4 /1)\n",
        "                    #print(tag)\n",
        "                    if textImgW >= 2 * min(len(tag), 16):  # avoid CTC error\n",
        "                        outBoxs.append(outBox)\n",
        "                        cropBoxs.append(cropBox)\n",
        "                        angles.append(angle)\n",
        "                        text_tags.append(tag[:16])\n",
        "                        recg_masks.append(1.)\n",
        "\n",
        "        for y, x in xy_in_poly:\n",
        "            point = np.array([x, y], dtype=np.float32)\n",
        "            # top\n",
        "            geo_map[y, x, 0] = point_dist_to_line(p0_rect, p1_rect, point) + 3\n",
        "            # right\n",
        "            geo_map[y, x, 1] = point_dist_to_line(p1_rect, p2_rect, point) + 3\n",
        "            # down\n",
        "            geo_map[y, x, 2] = point_dist_to_line(p2_rect, p3_rect, point) + 3\n",
        "            # left\n",
        "            geo_map[y, x, 3] = point_dist_to_line(p3_rect, p0_rect, point) + 3\n",
        "            # angle\n",
        "            geo_map[y, x, 4] = rotate_angle\n",
        "    if len(outBoxs) == 0:\n",
        "        outBoxs.append([0, 0, 2 * 4, 2 * 4]) # keep extract From sharedConv feature map not zero\n",
        "        cropBoxs.append([0, 0, 2 * 4, 2 * 4])\n",
        "        angles.append(0.)\n",
        "        text_tags.append([NUM_CLASSES - 2])\n",
        "        recg_masks.append(0.)\n",
        "\n",
        "    outBoxs = np.array(outBoxs, np.int32)\n",
        "    cropBoxs = np.array(cropBoxs, np.int32)\n",
        "    angles = np.array(angles, np.float32)\n",
        "\n",
        "    return score_map, geo_map, training_mask, (outBoxs, cropBoxs, angles), text_tags, recg_masks\n",
        "    \n",
        "\n",
        "###################################### TEXT DETECTION LOSS ############################################################\n",
        "\n",
        "# This is dice coefficient loss used to classify point as text and non text region \n",
        "\n",
        "#https://github.com/Masao-Taketani/FOTS_OCR/blob/master/module/Backbone_branch.py\n",
        "def dice_coefficient(y_true_cls, y_pred_cls,training_mask):\n",
        "    '''\n",
        "    dice loss\n",
        "    :param y_true_cls:\n",
        "    :param y_pred_cls:\n",
        "    :param training_mask:x\n",
        "    :return:\n",
        "    '''\n",
        "    eps = 10**-6\n",
        "    intersection = tf.reduce_sum(y_true_cls * y_pred_cls * training_mask)\n",
        "    \n",
        "    union = tf.reduce_sum(y_true_cls * training_mask) + tf.reduce_sum(y_pred_cls * training_mask) + eps\n",
        "    loss = 1. - (2 * intersection / union)\n",
        "    return loss\n",
        "\n",
        "\n",
        "#https://github.com/Masao-Taketani/FOTS_OCR/blob/master/module/Backbone_branch.py\n",
        "def rbox_loss(y_true_cls,y_true_geo,y_pred_geo,training_mask):\n",
        "    # d1 -> top, d2->right, d3->bottom, d4->left\n",
        "    d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=5, axis=3)\n",
        "    d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=5, axis=3)\n",
        "\n",
        "    area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt)\n",
        "    area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred)\n",
        "    w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred)\n",
        "    h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred)\n",
        "    area_intersect = w_union * h_union\n",
        "    area_union = area_gt + area_pred - area_intersect\n",
        "\n",
        "    L_AABB = -tf.math.log((area_intersect + 1.0) / (area_union + 1.0))\n",
        "    L_theta = 1 - tf.cos(theta_pred - theta_gt)\n",
        "\n",
        "    L_g = L_AABB +  50*L_theta\n",
        "    L_g=tf.squeeze(L_g,axis=3)\n",
        "\n",
        "    return tf.reduce_mean(L_g * y_true_cls * training_mask)\n",
        "\n",
        "\n",
        "#This class contains complete loss we have used for text Detection Branch\n",
        "class total_Loss(tf.keras.losses.Loss):\n",
        "    def __init__(self, from_logits=False,reduction=tf.keras.losses.Reduction.AUTO,name='Loss_layer'):\n",
        "        super(total_Loss, self).__init__(reduction=reduction, name=name)\n",
        "    def call(self, y_true, y_pred):\n",
        "    \n",
        "        #Getting geo_map and score_maps\n",
        "        y_true_cls=y_true[:,:,:,0]\n",
        "        y_pred_cls=y_pred[:,:,:,0]\n",
        "        y_pred_geo=y_pred[:,:,:,1:6]\n",
        "        y_true_geo=y_true[:,:,:,1:6]\n",
        "        training_mask=y_true[:,:,:,6]\n",
        "\n",
        "        #1. Dice Loss\n",
        "        dice_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask)\n",
        "        # we scale classification loss by factor of 0.01 to match the iou loss part\n",
        "        dice_loss *=0.01\n",
        "\n",
        "        rbox_loss_ = rbox_loss(y_true_cls,y_true_geo,y_pred_geo,training_mask)\n",
        "\n",
        "\n",
        "        return 100*(rbox_loss_ + dice_loss)\n",
        "\n",
        "################################################### NMS ##################################################################\n",
        "\n",
        "#These Are Function that will be used while converting geo_maps to score_maps and returns bounding boxes for image after nms\n",
        "#https://github.com/argman/EAST/blob/master/eval.py\n",
        "def sort_poly(p):\n",
        "    min_axis = np.argmin(np.sum(p, axis=1))\n",
        "    p = p[[min_axis, (min_axis+1)%4, (min_axis+2)%4, (min_axis+3)%4]]\n",
        "    if abs(p[0, 0] - p[1, 0]) > abs(p[0, 1] - p[1, 1]):\n",
        "        return p\n",
        "    else:\n",
        "        return p[[0, 3, 2, 1]]\n",
        "\n",
        "\n",
        "#https://github.com/argman/EAST/blob/master/locality_aware_nms.py\n",
        "def intersection(g, p):\n",
        "    g = Polygon(g[:8].reshape((4, 2)))\n",
        "    p = Polygon(p[:8].reshape((4, 2)))\n",
        "    if not g.is_valid or not p.is_valid:\n",
        "        return 0\n",
        "    inter = Polygon(g).intersection(Polygon(p)).area\n",
        "    union = g.area + p.area - inter\n",
        "    if union == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return inter/union\n",
        "\n",
        "\n",
        "def weighted_merge(g, p):\n",
        "    g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8])\n",
        "    g[8] = (g[8] + p[8])\n",
        "    return g\n",
        "\n",
        "\n",
        "def standard_nms(S, thres):\n",
        "    order = np.argsort(S[:, 8])[::-1]\n",
        "    keep = []\n",
        "    while order.size > 0:\n",
        "        i = order[0]\n",
        "        keep.append(i)\n",
        "        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n",
        "\n",
        "        inds = np.where(ovr <= thres)[0]\n",
        "        order = order[inds+1]\n",
        "\n",
        "    return S[keep]\n",
        "\n",
        "\n",
        "def nms_locality(polys, thres=0.3):\n",
        "    '''\n",
        "    :param polys: a N*9 numpy array. first 8 coordinates, then prob\n",
        "    :return: boxes after nms\n",
        "    '''\n",
        "    S = []\n",
        "    p = None\n",
        "  \n",
        "    for g in polys:\n",
        "        if p is not None and intersection(g, p) > thres:\n",
        "        \n",
        "            p = weighted_merge(g, p)\n",
        "        else:\n",
        "            if p is not None:\n",
        "                S.append(p)\n",
        "            p = g\n",
        "  \n",
        "    if p is not None:\n",
        "        S.append(p)\n",
        "\n",
        "    if len(S) == 0:\n",
        "        return np.array([])\n",
        "    \n",
        "    return standard_nms(np.array(S), thres)\n",
        "\n",
        "\n",
        "###################################################### TEXT RECOGNITION ###############################################\n",
        "\n",
        "#Preparing vocabulary for Text Recognition Branch\n",
        "#https://github.com/qjadud1994/CRNN-Keras/blob/master/Model_GRU.py\n",
        "TOTAL_CHARS = \" 0123456789abcdefghijklmnopqrstuvwxyzéABCDEFGHIJKLMNOPQRSTUVWXYZÉ´-~`<>'.:;^/|!?$%#@&*()[]{}_+=,\\\\\\\"\"\n",
        "NUM_CLASSES = len(TOTAL_CHARS) \n",
        "char_index={}\n",
        "index_char={}\n",
        "for i,val in enumerate(TOTAL_CHARS):\n",
        "    index_char[i+1]=val\n",
        "    char_index[val]=i+1\n",
        "    \n",
        "    \n",
        "#This class contains complete loss we have used for text Detection Branch\n",
        "class ctc_loss(tf.keras.losses.Loss):\n",
        "    def __init__(self, from_logits=False,reduction=tf.keras.losses.Reduction.AUTO,name='Loss_layer'):\n",
        "        super(ctc_loss, self).__init__(reduction=reduction, name=name)\n",
        "    def call(self, y_true, y_pred):\n",
        "\n",
        "        #Getting geo_map and score_maps\n",
        "        label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True)\n",
        "        return tf.keras.backend.ctc_batch_cost(y_true,y_pred,np.ones((128,1),'int32')*64,label_length)\n",
        "\n",
        "\n",
        "################################################### MODEL LOADING #######################################################\n",
        "\n",
        "model_resnet = load_model('/content/drive/MyDrive/detection_and_recognition_final_model_syntext/detect_fourth_model_resnet.h5', custom_objects={'total_Loss': total_Loss()})\n",
        "recg_model = load_model('/content/drive/MyDrive/detection_and_recognition_final_model_syntext/text_recognition_best_new_model.h5', custom_objects={'ctc_loss': ctc_loss()})\n",
        "\n",
        "################################################# FINAL PIPELINE ###########################################################\n",
        "\n",
        "#https://github.com/argman/EAST/blob/master/eval.py\n",
        "# Inference pipeline function used for generating predicted bounding boxes on image for text detection\n",
        "def FinalPipeline(img):\n",
        "  \n",
        "    #Text Detection\n",
        "    img=cv2.resize(img,(512,512))\n",
        "    img_for_predict = img\n",
        "    ii=model_resnet.predict(np.expand_dims(img,axis=0))\n",
        "    score_map=ii[0][:,:,0]\n",
        "    geo_map=ii[0][:,:,1:]\n",
        "\n",
        "    for ind in [0,1,2,3,4]:\n",
        "        geo_map[:,:,ind]*=score_map\n",
        "\n",
        "    #ROI Rotate  \n",
        "    score_map_thresh=0.5\n",
        "    box_thresh=0.1 \n",
        "    nms_thres=0.2\n",
        "    if len(score_map.shape) == 4:\n",
        "        score_map = score_map[0, :, :, 0]\n",
        "        geo_map = geo_map[0, :, :, :]\n",
        "\n",
        "\n",
        "    #filter the score map\n",
        "    xy_text = np.argwhere(score_map > score_map_thresh)\n",
        "\n",
        "    # sort the text boxes via the y axis\n",
        "    xy_text = xy_text[np.argsort(xy_text[:, 0])]\n",
        "\n",
        "    # restore\n",
        "    text_box_restored = restore_rectangle(xy_text[:, ::-1], geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2\n",
        "    boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32)\n",
        "    boxes[:, :8] = text_box_restored.reshape((-1, 8))\n",
        "    boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]]\n",
        "    boxes = nms_locality(boxes.astype(np.float64), nms_thres)\n",
        "\n",
        "  # here we filter some low score boxes by the average score map, this is different from the orginal paper\n",
        "    for i, box in enumerate(boxes):\n",
        "        mask = np.zeros_like(score_map, dtype=np.uint8)\n",
        "        cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32), 1)\n",
        "        boxes[i, 8] = cv2.mean(score_map, mask)[0]\n",
        "\n",
        "        if i==4:\n",
        "            break\n",
        "\n",
        "    if len(boxes)>0:\n",
        "        boxes = boxes[boxes[:, 8] > box_thresh]\n",
        "    boxes[:,:8:2] = np.clip(boxes[:,:8:2], 0, 512 - 1)\n",
        "    boxes[:,1:8:2] = np.clip(boxes[:,1:8:2], 0, 512 - 1)  \n",
        "\n",
        "    res = []\n",
        "    result = []\n",
        "    if len(boxes)>0:\n",
        "        for box in boxes:\n",
        "            box_ =  box[:8].reshape((4, 2))\n",
        "            if np.linalg.norm(box_[0] - box_[1]) < 8 or np.linalg.norm(box_[3]-box_[0]) < 8:\n",
        "                continue\n",
        "            result.append(box_)\n",
        "    res.append(np.array(result, np.float32))   \n",
        "\n",
        "    box_index = []\n",
        "    brotateParas = []\n",
        "    filter_bsharedFeatures = []\n",
        "    for i in range(len(res)):\n",
        "        rotateParas = []\n",
        "        rboxes=res[i]\n",
        "        txt=[]\n",
        "        for j, rbox in enumerate(rboxes):\n",
        "            para = restore_roiRotatePara(rbox)\n",
        "            if para and min(para[1][2:]) > 8:\n",
        "                rotateParas.append(para)\n",
        "                box_index.append((i, j))\n",
        "\n",
        "    pts=[]   \n",
        "    #Text Recognition (From boxes given by Text Detection+ROI Rotate) \n",
        "    if len(rotateParas) > 0:\n",
        "        for num in range(len(rotateParas)):\n",
        "            text=\"\"\n",
        "            out=rotateParas[num][0]\n",
        "            crop=rotateParas[num][1]\n",
        "            points=np.array([[out[0],out[1]],[out[0]+out[2],out[1]],[out[0]+out[2],out[1]+out[3]],[out[0],out[1]+out[3]]])\n",
        "            angle=rotateParas[num][2] \n",
        "            img1=tf.image.crop_to_bounding_box(img,out[1],out[0],out[3],out[2])\n",
        "            img2=tf.keras.preprocessing.image.random_rotation(img1,angle)\n",
        "            img2=tf.image.crop_to_bounding_box(img2,crop[1],crop[0],crop[3],crop[2]).numpy()\n",
        "            img2=cv2.resize(img2,(128,64))\n",
        "            img2=cv2.detailEnhance(img2)\n",
        "            ii=recg_model.predict(np.expand_dims(img2,axis=0))\n",
        "            arr=tf.keras.backend.ctc_decode(ii,np.ones((1),'int8')*64,)\n",
        "            for val in arr[0][0].numpy()[0]:\n",
        "                if val==-1:\n",
        "                    break\n",
        "                else:\n",
        "                    text+=index_char[val]\n",
        "            txt.append(spell(text))\n",
        "            pts.append(points)\n",
        "    \n",
        "    # 4. Labeling detected and Recognized Text in Image\n",
        "  \n",
        "    for i in range(len(txt)):\n",
        "        cv2.polylines(img,[pts[i]],isClosed=True,color=(255,255,0),thickness=2)\n",
        "        cv2.putText(img,txt[i],(pts[i][0][0],pts[i][0][1]),cv2.FONT_HERSHEY_SIMPLEX,0.8, (0, 0, 0), 2)\n",
        "    \n",
        "    return img, txt\n",
        "\n",
        "st.title('scene TEXT Detection and Recognition')\n",
        "st.header('Enter the image')\n",
        "img=st.file_uploader('upload a image')\n",
        "if img:    \n",
        "  st.image(img,width=512)\n",
        "  image = Image.open(img)\n",
        "  img = np.array(image)\n",
        "  im,txt = FinalPipeline(img)\n",
        "  im=cv2.resize(im,(512,400))\n",
        "  st.header('Resulted image')\n",
        "  st.image(im)\n",
        "  st.write(\"Predicted Text is :\",txt)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8oBRW-h4smk"
      },
      "source": [
        "Steps for building web app using streamlit and ngrok:\n",
        "- Register: https://dashboard.ngrok.com/get-started/setup\n",
        "- Create An Account on Ngrok which was from above link\n",
        "- Get Your Authentication Tokens\n",
        "- Copy and paste your authtoken in \"!ngrok authtoken\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdWjAYU-4r5y",
        "outputId": "7b941486-8c2e-4af7-8417-241d6a7c5e04"
      },
      "source": [
        "!ngrok authtoken 1vZxlrOzTU2fxUUAFuxvzIeG2gH_6fLGBQw8GVxYJMoGQQfr3\n",
        "\n",
        "# !ngrok authtoken **********************************************#Copy paste your token\n",
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(port=80)\n",
        "print(public_url)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n",
            "NgrokTunnel: \"http://cc7c6184d98f.ngrok.io\" -> \"http://localhost:80\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaL3DFmS45oy",
        "outputId": "429e5962-968e-4e9e-ca6b-b095497a7ea7"
      },
      "source": [
        "!export STREAMLIT_SERVER_PORT=80\n",
        "#running Script using streamlit:\n",
        "!streamlit run /content/app.py --server.port 80"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:80\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.91.181.139:80\u001b[0m\n",
            "\u001b[0m\n",
            "2021-07-25 06:20:53.954658: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-07-25 06:20:55.476335: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-25 06:20:55.488984: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-07-25 06:20:55.489035: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d67855931452): /proc/driver/nvidia/version does not exist\n",
            "2021-07-25 06:21:13.383807: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-07-25 06:21:13.384210: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2249995000 Hz\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVf_3x4fd89_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}